---
title: "Custom Estimators"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Custom Estimators}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Users can also implement their own custom Estimator by configuring the layers and architecture of the Estimator. The Estimator's architecture is configured using a user-defined `model_fn`, a function which builds a TensorFlow graph and returns necessary information to train a model, evaluate it, and predict with it. Users writing custom estimators to implement custom model architecture only have to implement this function to specify the layers of the custom Estimator. It is possible, and in fact, common, that `model_fn` contains regular TensorFlow without using any other part of our framework.  It is often the case because existing models are being adapted or converted to be implemented in terms of an estimator.

Users define the model architecture in a custom model function `custom_model_fn` that contains the following arguments in the signature that users can grab to define customized handling conditionally:

* features and labels of the model.
* mode that contains the different modes of a model, such as training, inference, or evaluation.
* params that contains the tuning parameters in a model.
* config that represents the `RunConfig` objects used in a model, including GPU percentages, cluster information, etc.

The `custom_model_fn()` function should return an `estimator_spec(predictions, loss, train_op, mode)` that contains the predictions, losses, training op, and mode.

The following example demonstrates the construction and fitting of a custom estimator that has custom architectures. 

Firstly, let's define the input using `input_fn()`:

``` r
constructed_input_fn <- input_fn(
	object = iris,
	response = "Species",
	features = c(
	  "Sepal.Length",
	  "Sepal.Width",
	  "Petal.Length",
	  "Petal.Width"),
	batch_size = 10L
)
```

Next, we define the custom model function:

``` r

custom_model_fn <- function(features, labels, mode, params, config) {
	  # Create three fully connected layers respectively of size 10, 20, and 10 with
    # each layer having a dropout probability of 0.1.
    logits <- features %>%
      tf$contrib$layers$stack(
        tf$contrib$layers$fully_connected, c(10L, 20L, 10L),
        normalizer_fn = tf$contrib$layers$dropout,
        normalizer_params = list(keep_prob = 0.9)) %>%
      tf$contrib$layers$fully_connected(3L, activation_fn = NULL) # Compute logits (1 per class) and compute loss.
    
    predictions <- list(
      class = tf$argmax(logits, 1L),
      prob = tf$nn$softmax(logits))
    
    # Return estimator_spec early with NULL loss and train_op during inference mode
    if(mode == "infer"){
      return(estimator_spec(
      mode = mode, predictions = predictions, loss = NULL, train_op = NULL))
    }
    
    labels <- tf$one_hot(labels, 3L)
    loss <- tf$losses$softmax_cross_entropy(labels, logits)
    
    # Create a tensor for training op.
    train_op <- tf$contrib$layers$optimize_loss(
      loss,
      tf$contrib$framework$get_global_step(),
      optimizer = 'Adagrad',
      learning_rate = 0.1)
    
    return(estimator_spec(mode = mode, predictions = predictions, loss = loss, train_op = train_op))
}

# Initialize and fit the model using the the custom model function we defined
# and the constructed_input_fn that represents the input data source.  
classifier <- estimator(model_fn = custom_model_fn)
classifier <- train(classifier, input_fn = constructed_input_fn, steps = 2L)
```

Note that the above code contains a lot of `$`s. It is unnecessary to create wrapper APIs for every methods that users might use, e.g. `tf$contrib$layers$optimize_loss`, since custom models are designed to be flexible and extensible so users can insert any arbitrary low level TensorFlow APIs.

Users can then supply new data in `input_fn` to `predict()` and make predictions using the trained model: 

``` r
predictions <- predict(classifier, input_fn = constructed_input_fn)
```

Since our predictions is defined as a list of two named items in the custom model function like follows:

``` r
predictions <- list(
  class = tf$argmax(logits, 1L),
  prob = tf$nn$softmax(logits))
```

we can then loop through each prediction and collect the predicted classes and probabilities like the following:

``` r
# extract predicted classes
predicted_classes <- lapply(predictions, function(prediction) prediction$class)

# extract the raw probabilities for each new row of data and for each class
predicted_raw_probs <- lapply(predictions, function(prediction) prediction$prob)

```

Users can also pass `model_dir` which is the directory to save model parameters, graph and etc. This can also be used to load checkpoints from the directory into a estimator to continue training a previously saved model. Users can also pass a `config` to canned Estimator or custom Estimator that specify the model run-time configuration, such as cluster information, GPU fractions, etc.
