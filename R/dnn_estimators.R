#' Deep Neural Networks
#'
#' Create a deep neural network (DNN) estimator.
#'
#' @inheritParams estimators
#' 
#' @param hidden_units An integer vector, indicating the number of hidden
#'   units in each layer. All layers are fully connected. For example,
#'   `c(64, 32)` means the first layer has 64 nodes, and the second layer
#'   has 32 nodes.
#' @param feature_columns An \R list containing all of the feature columns used
#'   by the model (typically, generated by [feature_columns()]).
#' @param label_vocabulary A list of strings represents possible label values.
#'   If given, labels must be string type and have any value in
#'   `label_vocabulary`. If it is not given, that means labels are already
#'   encoded as integer or float within [0, 1] for `n_classes=2` and encoded as
#'   integer values in {0, 1,..., n_classes-1} for `n_classes`>2 . Also there
#'   will be errors if vocabulary is not provided and labels are string.
#' @param optimizer Either the name of the optimizer to be used when training the model,
#'   or a `tf.Optimizer` instance. Defaults to the Adagrad optimizer.
#' @param activation_fn The activation function to apply to each layer. Defaults
#'   to the **re**ctified **l**inear **u**nit activation function (`tf$nn$relu`).
#' @param dropout When not `NULL`, the probability we will drop out a given
#'   coordinate.
#'
#' @family canned estimators
#' @name dnn_estimators
NULL

#' @inheritParams dnn_estimators
#' @name dnn_estimators
#' @export
dnn_regressor <- function(hidden_units,
                          feature_columns,
                          model_dir = NULL,
                          label_dimension = 1L,
                          weight_column = NULL,
                          optimizer = "Adagrad",
                          activation_fn = tf$nn$relu,
                          dropout = NULL,
                          input_layer_partitioner = NULL,
                          config = NULL)
{
  estimator <- py_suppress_warnings(
    tf$estimator$DNNRegressor(
      hidden_units = as.integer(hidden_units),
      feature_columns = ensure_nullable_list(feature_columns),
      model_dir = resolve_model_dir(model_dir),
      label_dimension = as.integer(label_dimension),
      weight_column = weight_column,
      optimizer = optimizer,
      activation_fn = activation_fn,
      dropout = dropout,
      input_layer_partitioner = input_layer_partitioner,
      config = config
    )
  )

  tf_regressor(estimator, "dnn_regressor")
}

#' @inheritParams dnn_estimators
#' @name dnn_estimators
#' @export
dnn_classifier <- function(hidden_units,
                           feature_columns,
                           model_dir = NULL,
                           n_classes = 2L,
                           weight_column = NULL,
                           label_vocabulary = NULL,
                           optimizer = "Adagrad",
                           activation_fn = tf$nn$relu,
                           dropout = NULL,
                           input_layer_partitioner = NULL,
                           config = NULL)
{
  estimator <- py_suppress_warnings(
    tf$estimator$DNNClassifier(
      hidden_units = as.integer(hidden_units),
      feature_columns = ensure_nullable_list(feature_columns),
      model_dir = resolve_model_dir(model_dir),
      n_classes = as.integer(n_classes),
      weight_column = weight_column,
      label_vocabulary = label_vocabulary,
      optimizer = optimizer,
      activation_fn = activation_fn,
      dropout = dropout,
      input_layer_partitioner = input_layer_partitioner,
      config = config
    )
  )

  tf_classifier(estimator, "dnn_classifier")
}

