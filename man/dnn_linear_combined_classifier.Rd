% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dnn_linear_combined_estimators.R
\name{dnn_linear_combined_classifier}
\alias{dnn_linear_combined_classifier}
\title{An estimator for TensorFlow Linear and DNN joined classification models.}
\usage{
dnn_linear_combined_classifier(model_dir = NULL,
  linear_feature_columns = NULL, linear_optimizer = "Ftrl",
  dnn_feature_columns = NULL, dnn_optimizer = "Adagrad",
  dnn_hidden_units = NULL, dnn_activation_fn = tf$nn$relu,
  dnn_dropout = NULL, n_classes = 2L, weight_column = NULL,
  label_vocabulary = NULL, input_layer_partitioner = NULL, config = NULL)
}
\arguments{
\item{model_dir}{Directory to save model parameters, graph and etc. This can
also be used to load checkpoints from the directory into a estimator to
continue training a previously saved model.}

\item{linear_feature_columns}{An iterable containing all the feature columns
used by linear part of the model. All items in the set must be instances of
classes derived from \code{FeatureColumn}.}

\item{linear_optimizer}{An instance of \code{tf.Optimizer} used to apply gradients
to the linear part of the model. Defaults to FTRL optimizer.}

\item{dnn_feature_columns}{An iterable containing all the feature columns
used by deep part of the model. All items in the set must be instances of
classes derived from \code{FeatureColumn}.}

\item{dnn_optimizer}{An instance of \code{tf.Optimizer} used to apply gradients to
the deep part of the model. Defaults to Adagrad optimizer.}

\item{dnn_hidden_units}{List of hidden units per layer. All layers are fully
connected.}

\item{dnn_activation_fn}{Activation function applied to each layer. If NULL,
will use \code{tf$nn$relu}.}

\item{dnn_dropout}{When not NULL, the probability we will drop out a given
coordinate.}

\item{n_classes}{Number of label classes. Defaults to 2, namely binary
classification. Must be > 1.}

\item{weight_column}{A string or a \code{_NumericColumn} created by
\code{numeric_column} defining feature column representing weights. It is used
to down weight or boost examples during training. It will be multiplied by
the loss of the example. If it is a string, it is used as a key to fetch
weight tensor from the \code{features}. If it is a \code{_NumericColumn}, raw tensor
is fetched by key \code{weight_column.key}, then weight_column.normalizer_fn is
applied on it to get weight tensor.}

\item{label_vocabulary}{A list of strings represents possible label values.
If given, labels must be string type and have any value in
\code{label_vocabulary}. If it is not given, that means labels are already
encoded as integer or float within \link{0, 1} for \code{n_classes=2} and encoded as
integer values in {0, 1,..., n_classes-1} for \code{n_classes}>2 . Also there
will be errors if vocabulary is not provided and labels are string.}

\item{input_layer_partitioner}{Partitioner for input layer. Defaults to
\code{min_max_variable_partitioner} with \code{min_slice_size} 64 << 20.}

\item{config}{RunConfig object to configure the runtime settings.}
}
\description{
Note: This estimator is also known as wide-n-deep.
}
\seealso{
Other canned estimators: \code{\link{dnn_estimators}},
  \code{\link{dnn_linear_combined_regressor}},
  \code{\link{dynamic_rnn}},
  \code{\link{linear_classifier}},
  \code{\link{linear_regressor}},
  \code{\link{state_saving_rnn}}
}
